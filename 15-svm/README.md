### Before

 * Read the very friendly Chapter 18 (What's Your Vector, Victor?) on support vector machines from [Introduction to Data Science](http://jsresearch.net/). (It gives its example in `R`.)

Optional:

* Read this more exacting article, [A User's Guide to Support Vector Machines](http://pyml.sourceforge.net/doc/howto.pdf). (The class slides draw from this material.)


### Questions

 * Imagine an example dataset with two continuous features corresponding to axes in the plane, and binary label. How well would each of the classification algorithms we've seen so far perform with this data?
 * What other thoughts, comments, concerns, and questions do you have? What's on your mind?


### During

Application presentation.

Question review.

[Slides](slides.pdf) on support vector machines.

Demonstrate a simple numerical example for the discriminant function. Use the training set ((1, 3), +1), ((2, 2), +1), ((1, 2), -1), ((2, 1), -1). Draw a separating hyperplane, get orthogonal vector (_w_), multiply matrices, find a _b_, interpret a little.

Reiterate two 'parts' of SVM:
 * maximum margin hyperplane
 * kernel function / kernel trick


### After

Optional:
